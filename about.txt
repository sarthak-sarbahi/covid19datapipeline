Use case inspiration: https://medium.com/dev-genius/data-engineering-project-analyzing-covid-data-3db7c15c57dc

Prerequisites:
1. Created a service account with following roles:
    - BigQuery Admin
    - Composer Administrator
    - Dataproc Administrator
    - Editor
    - Storage Admin
2. Python file in GCS for Dataproc to execute (main.py)
3. Create dataset in BigQuery

Flow:
1. Fetch data in JSON from an API (https://covidtracking.com/data/api/version-2)
2. Write to a bucket in Google Cloud Storage
3. Spin up Spark job to transform data using Dataproc
4. Transformed data written to BigQuery
5. Spark cluster auto-terminates after transformation

Execute: python3 src/main.py

Future scope:
1. Use Apache Airflow to orchestrate the data pipeline