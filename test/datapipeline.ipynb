{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# from pyspark.sql import SparkSession\n",
    "# import pyspark.sql.functions as F\n",
    "# import pyspark.sql.types as T\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findspark.init()\n",
    "# spark = (SparkSession\n",
    "#         .builder\n",
    "#         .appName(\"transformations\")\n",
    "#         .master(\"local[*]\")\n",
    "#         .getOrCreate()\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket covid19data_ss already exists.\n",
      "Bucket covid19data_ss_bq_write already exists.\n",
      "JSON data saved to gs://covid19data_ss/covidtrackingdata.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create a bucket (if not exists) in Google Cloud Storage\n",
    "Download JSON file in bucket (created in preceding step)\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from google.cloud import storage\n",
    "import json\n",
    "import io\n",
    "\n",
    "API_URL = \"https://api.covidtracking.com/v2/us/daily.json\"\n",
    "GCP_BUCKET_NAME = \"covid19data_ss\"\n",
    "BQ_WRITE_BUCKET_NAME = \"covid19data_ss_bq_write\"\n",
    "DATA_DESTINATION_NAME = \"covidtrackingdata.json\"\n",
    "SERVICE_ACCOUNT_JSON_PATH = \"/home/infernape/gcp-projects/covid19datapipeline/service_account_secrets.json\"\n",
    "PROJECT_ID = \"sunlit-vortex-394519\"\n",
    "client = storage.Client.from_service_account_json(SERVICE_ACCOUNT_JSON_PATH)\n",
    "\n",
    "# Create the bucket if it doesn't exist\n",
    "def create_bucket_if_not_exists(bucket_name, project_id):\n",
    "    try:\n",
    "        client.get_bucket(bucket_name)\n",
    "        print(f\"Bucket {bucket_name} already exists.\")\n",
    "    except Exception as e:\n",
    "        bucket = client.create_bucket(bucket_name, project=project_id)\n",
    "        print(f\"Bucket {bucket_name} created.\")\n",
    "\n",
    "create_bucket_if_not_exists(GCP_BUCKET_NAME, PROJECT_ID)\n",
    "create_bucket_if_not_exists(BQ_WRITE_BUCKET_NAME, PROJECT_ID)\n",
    "\n",
    "# Fetch JSON data from the API\n",
    "response = requests.get(API_URL)\n",
    "response.raise_for_status()\n",
    "data = response.json()[\"data\"]\n",
    "json_data = json.dumps(data)\n",
    "\n",
    "# Create a bytes stream and write the JSON string to it\n",
    "bytes_stream = io.BytesIO()\n",
    "bytes_stream.write(json_data.encode('utf-8'))\n",
    "bytes_stream.seek(0)\n",
    "\n",
    "# Initialize the GCS client with service account credentials\n",
    "bucket = client.get_bucket(GCP_BUCKET_NAME)\n",
    "blob = bucket.blob(DATA_DESTINATION_NAME)\n",
    "\n",
    "# delete JSON file if already exists\n",
    "if blob.exists():\n",
    "    blob.delete()\n",
    "    print(f\"Blob {DATA_DESTINATION_NAME} deleted from {GCP_BUCKET_NAME}.\")\n",
    "\n",
    "# Save the JSON data to GCS\n",
    "blob.upload_from_file(bytes_stream, content_type='application/json')\n",
    "\n",
    "print(f\"JSON data saved to gs://{GCP_BUCKET_NAME}/{DATA_DESTINATION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cluster...\n",
      "Cluster running...\n",
      "Submitting job...\n",
      "Job completed...\n",
      "Deleting cluster...\n",
      "Cluster deleted...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Execute spark job programmatically\n",
    "Spin up dataproc cluster and submit job for execution\n",
    "Delete the dataproc cluster upon successful completion of job\n",
    "\"\"\"\n",
    "\n",
    "from google.cloud import dataproc_v1 as dataproc\n",
    "from google.cloud import storage\n",
    "from time import sleep\n",
    "\n",
    "PROJECT_ID = \"sunlit-vortex-394519\"\n",
    "CLUSTER_NAME = \"cluster-07bc\"\n",
    "REGION = \"us-central1\"\n",
    "DATAPROC_JOB_FILE = \"gs://covid19data_ss/scripts/main.py\"\n",
    "SERVICE_ACCOUNT_JSON_PATH = \"/home/infernape/gcp-projects/covid19datapipeline/service_account_secrets.json\"\n",
    "\n",
    "# Initialize Dataproc and Storage clients\n",
    "cluster_client = dataproc.ClusterControllerClient.from_service_account_file(SERVICE_ACCOUNT_JSON_PATH,client_options={'api_endpoint': f'{REGION}-dataproc.googleapis.com:443'})\n",
    "job_client = dataproc.JobControllerClient.from_service_account_file(SERVICE_ACCOUNT_JSON_PATH,client_options={'api_endpoint': f'{REGION}-dataproc.googleapis.com:443'})\n",
    "\n",
    "# Create cluster config\n",
    "cluster_config = {\n",
    "  \"project_id\": PROJECT_ID,\n",
    "  \"cluster_name\": CLUSTER_NAME,\n",
    "  \"config\": {\n",
    "    \"config_bucket\": \"\",\n",
    "    \"gce_cluster_config\": {\n",
    "      \"service_account_scopes\": [\n",
    "        \"https://www.googleapis.com/auth/cloud-platform\"\n",
    "      ],\n",
    "      \"network_uri\": \"default\",\n",
    "      \"subnetwork_uri\": \"\",\n",
    "      \"internal_ip_only\": False,\n",
    "      \"zone_uri\": \"\",\n",
    "      \"metadata\": {},\n",
    "      \"tags\": [],\n",
    "      \"shielded_instance_config\": {\n",
    "        \"enable_secure_boot\": False,\n",
    "        \"enable_vtpm\": False,\n",
    "        \"enable_integrity_monitoring\": False\n",
    "      }\n",
    "    },\n",
    "    \"master_config\": {\n",
    "      \"num_instances\": 1,\n",
    "      \"machine_type_uri\": \"n2-standard-4\",\n",
    "      \"disk_config\": {\n",
    "        \"boot_disk_type\": \"pd-standard\",\n",
    "        \"boot_disk_size_gb\": 500,\n",
    "        \"num_local_ssds\": 0\n",
    "      },\n",
    "      \"min_cpu_platform\": \"\",\n",
    "      \"image_uri\": \"\"\n",
    "    },\n",
    "    \"software_config\": {\n",
    "      \"image_version\": \"2.1-debian11\",\n",
    "      \"properties\": {\n",
    "        \"dataproc:dataproc.allow.zero.workers\": \"true\"\n",
    "      },\n",
    "      \"optional_components\": []\n",
    "    },\n",
    "    \"lifecycle_config\": {},\n",
    "    \"initialization_actions\": [],\n",
    "    \"encryption_config\": {\n",
    "      \"gce_pd_kms_key_name\": \"\"\n",
    "    },\n",
    "    \"autoscaling_config\": {\n",
    "      \"policy_uri\": \"\"\n",
    "    },\n",
    "    \"endpoint_config\": {\n",
    "      \"enable_http_port_access\": False\n",
    "    },\n",
    "    \"security_config\": {\n",
    "      \"kerberos_config\": {}\n",
    "    }\n",
    "  },\n",
    "  \"labels\": {},\n",
    "  \"status\": {},\n",
    "  \"status_history\": [\n",
    "    {}\n",
    "  ],\n",
    "  \"metrics\": {}\n",
    "}\n",
    "\n",
    "# Create cluster\n",
    "request = dataproc.CreateClusterRequest(\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        cluster=cluster_config,\n",
    ")\n",
    "try:\n",
    "    print(\"Creating cluster...\")\n",
    "    operation = cluster_client.create_cluster(request = request)\n",
    "    result = operation.result()\n",
    "except Exception as e:\n",
    "    print(e) \n",
    "\n",
    "# Ensure cluster is up before submitting a job\n",
    "cluster_request = dataproc.GetClusterRequest(\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        cluster_name=CLUSTER_NAME,\n",
    ")   \n",
    "while True:\n",
    "    cluster_info = cluster_client.get_cluster(request = cluster_request)\n",
    "    if cluster_info.status.state.name == 'RUNNING':\n",
    "        print(\"Cluster running...\")\n",
    "        break\n",
    "    sleep(10)\n",
    "    \n",
    "# Submit job to Dataproc\n",
    "print(\"Submitting job...\")\n",
    "job_config = {\n",
    "    'placement': {\n",
    "        'cluster_name': CLUSTER_NAME\n",
    "    },\n",
    "    'pyspark_job': {\n",
    "        'main_python_file_uri': DATAPROC_JOB_FILE\n",
    "    }\n",
    "}\n",
    "submit_job_request = dataproc.SubmitJobRequest(\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        job = job_config\n",
    ")\n",
    "job = job_client.submit_job_as_operation(request = submit_job_request)\n",
    "job_id = job.result().reference.job_id\n",
    "\n",
    "# Wait for job completion\n",
    "job_request = dataproc.GetJobRequest(\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        job_id=job_id,\n",
    ")\n",
    "while True:\n",
    "    job_info = job_client.get_job(request = job_request)\n",
    "    if job_info.status.state.name == 'DONE':\n",
    "        print(\"Job completed...\")\n",
    "        break\n",
    "    sleep(10)\n",
    "\n",
    "# Delete the cluster\n",
    "print(\"Deleting cluster...\")\n",
    "operation = cluster_client.delete_cluster(\n",
    "    request={\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"region\": REGION,\n",
    "        \"cluster_name\": CLUSTER_NAME,\n",
    "    }\n",
    ")\n",
    "operation.result()\n",
    "print(\"Cluster deleted...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Read downloaded JSON file from GCS bucket\n",
    "# Convert into Spark dataframe and perform transformations\n",
    "# \"\"\"\n",
    "\n",
    "# # JSON_FILE_PATH = \"/home/infernape/gcp-projects/covid19datapipeline/test/covidtrackingdata.json\"\n",
    "\n",
    "# # df = spark.read.json(JSON_FILE_PATH)\n",
    "\n",
    "# INPUT_FILE = \"/home/infernape/gcp-projects/covid19datapipeline/test/test.txt\"\n",
    "# OUTPUT_FILE = \"/home/infernape/gcp-projects/covid19datapipeline/test/covidtrackingdata.json\"\n",
    "\n",
    "# rdd = spark.sparkContext.textFile(INPUT_FILE)\n",
    "\n",
    "# def replace_content(line):\n",
    "#     line = line.replace(\"'\", '\"')  # Convert single quotes to double quotes\n",
    "#     line = line.replace(\": None\", \": null\")  # Convert None to null\n",
    "#     return line\n",
    "\n",
    "# processed_rdd = rdd.map(replace_content)\n",
    "# df = spark.read.json(processed_rdd)\n",
    "# df.show(1)\n",
    "# # df.write.json(OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import dataproc_v1 as dataproc\n",
    "# from google.cloud import storage\n",
    "# from time import sleep\n",
    "\n",
    "# # TODO: Replace these with your values\n",
    "# PROJECT_ID = 'sunlit-vortex-394519'\n",
    "# REGION = 'us-central1'\n",
    "# CLUSTER_NAME = 'covidprocess'\n",
    "# GCS_BUCKET = 'covid19data_ss'\n",
    "# GCS_INPUT_PATH = f'gs://{GCS_BUCKET}/covidtrackingdata.txt'\n",
    "# GCS_OUTPUT_PATH = f'gs://{GCS_BUCKET}/output.json'\n",
    "# SERVICE_ACCOUNT_JSON_PATH = 'path_to_service_account_key.json'\n",
    "\n",
    "# # Initialize Dataproc and Storage clients\n",
    "# cluster_client = dataproc.ClusterControllerClient()\n",
    "# job_client = dataproc.JobControllerClient()\n",
    "\n",
    "# # Create cluster config\n",
    "# cluster_config = {\n",
    "#     'project_id': PROJECT_ID,\n",
    "#     'cluster_name': CLUSTER_NAME,\n",
    "#     'config': {\n",
    "#         'master_config': {\n",
    "#             'num_instances': 1,\n",
    "#             'machine_type_uri': 'n1-standard-1'\n",
    "#         },\n",
    "#         'worker_config': {\n",
    "#             'num_instances': 0\n",
    "#         },\n",
    "#         'gce_cluster_config': {\n",
    "#             'service_account_scopes': [\n",
    "#                 'https://www.googleapis.com/auth/cloud-platform'\n",
    "#             ]\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Create cluster\n",
    "# print(\"Creating cluster...\")\n",
    "# cluster = cluster_client.create_cluster(PROJECT_ID, REGION, cluster_config)\n",
    "# cluster_id = cluster.cluster_uuid\n",
    "\n",
    "# # Ensure cluster is up before submitting a job\n",
    "# while True:\n",
    "#     cluster_info = cluster_client.get_cluster(PROJECT_ID, REGION, CLUSTER_NAME)\n",
    "#     if cluster_info.status.state.name == 'RUNNING':\n",
    "#         break\n",
    "#     sleep(10)\n",
    "\n",
    "# # PySpark script to process data\n",
    "# pyspark_script = f\"\"\"\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"ProcessingData\").getOrCreate()\n",
    "\n",
    "# # Read data\n",
    "# rdd = spark.sparkContext.textFile(\"{GCS_INPUT_PATH}\")\n",
    "\n",
    "# # Process data\n",
    "# processed_rdd = rdd.map(lambda x: x.replace(\"'\", '\"').replace(\"None\", \"null\"))\n",
    "\n",
    "# # Convert RDD to DataFrame (assuming each line is valid JSON after transformation)\n",
    "# df = spark.read.json(processed_rdd)\n",
    "\n",
    "# # Save as JSON\n",
    "# df.write.json(\"{GCS_OUTPUT_PATH}\")\n",
    "\n",
    "# spark.stop()\n",
    "# \"\"\"\n",
    "\n",
    "# # Save PySpark script to GCS\n",
    "# gcs_client = storage.Client.from_service_account_json(SERVICE_ACCOUNT_JSON_PATH)\n",
    "# bucket = gcs_client.bucket(GCS_BUCKET)\n",
    "# blob = bucket.blob('script/process_data.py')\n",
    "# blob.upload_from_string(pyspark_script)\n",
    "\n",
    "# # Submit job to Dataproc\n",
    "# print(\"Submitting job...\")\n",
    "# job_config = {\n",
    "#     'placement': {\n",
    "#         'cluster_name': CLUSTER_NAME\n",
    "#     },\n",
    "#     'pyspark_job': {\n",
    "#         'main_python_file_uri': f'gs://{GCS_BUCKET}/script/process_data.py'\n",
    "#     }\n",
    "# }\n",
    "# job = job_client.submit_job_as_operation(PROJECT_ID, REGION, job_config)\n",
    "# job_id = job.name\n",
    "\n",
    "# # Wait for job completion\n",
    "# while True:\n",
    "#     job_info = job_client.get_job(PROJECT_ID, REGION, job_id)\n",
    "#     if job_info.status.state.name == 'DONE':\n",
    "#         break\n",
    "#     sleep(10)\n",
    "\n",
    "# # Delete the cluster\n",
    "# print(\"Deleting cluster...\")\n",
    "# cluster_client.delete_cluster(PROJECT_ID, REGION, CLUSTER_NAME)\n",
    "# print(\"Cluster deleted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# INPUT_FILE = \"/home/infernape/gcp-projects/covid19datapipeline/test/test.txt\"\n",
    "# with open(INPUT_FILE, 'r') as file:\n",
    "#      file_content = file.read()\n",
    "        \n",
    "# file_content_trnsfm = file_content.replace(\"'\",'\"').replace(\": None\", \": null\")\n",
    "# # print(file_content_trnsfm)\n",
    "\n",
    "# data = json.loads(file_content_trnsfm)\n",
    "\n",
    "# with open('output_file.json', 'w') as outfile:\n",
    "#     json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"date\", StringType(), True),\n",
    "#     StructField(\"states\", IntegerType(), True),\n",
    "#     StructField(\"cases\", StructType([\n",
    "#         StructField(\"total\", StructType([\n",
    "#             StructField(\"value\", IntegerType(), True),\n",
    "#             StructField(\"calculated\", StructType([\n",
    "#                 StructField(\"population_percent\", DoubleType(), True),\n",
    "#                 StructField(\"change_from_prior_day\", IntegerType(), True),\n",
    "#                 StructField(\"seven_day_change_percent\", DoubleType(), True)\n",
    "#             ]))\n",
    "#         ]))\n",
    "#     ])),\n",
    "#     StructField(\"testing\", StructType([\n",
    "#         StructField(\"total\", StructType([\n",
    "#             StructField(\"value\", IntegerType(), True),\n",
    "#             StructField(\"calculated\", StructType([\n",
    "#                 StructField(\"population_percent\", DoubleType(), True),\n",
    "#                 StructField(\"change_from_prior_day\", IntegerType(), True),\n",
    "#                 StructField(\"seven_day_change_percent\", DoubleType(), True)\n",
    "#             ]))\n",
    "#         ]))\n",
    "#     ])),\n",
    "#     StructField(\"outcomes\", StructType([\n",
    "#         StructField(\"hospitalized\", StructType([\n",
    "#             StructField(\"currently\", StructType([\n",
    "#                 StructField(\"value\", IntegerType(), True),\n",
    "#                 StructField(\"calculated\", StructType([\n",
    "#                     StructField(\"population_percent\", DoubleType(), True),\n",
    "#                     StructField(\"change_from_prior_day\", IntegerType(), True),\n",
    "#                     StructField(\"seven_day_change_percent\", DoubleType(), True),\n",
    "#                     StructField(\"seven_day_average\", IntegerType(), True)\n",
    "#                 ]))\n",
    "#             ])),\n",
    "#             StructField(\"in_icu\", StructType([\n",
    "#                 StructField(\"currently\", StructType([\n",
    "#                     StructField(\"value\", IntegerType(), True),\n",
    "#                     StructField(\"calculated\", StructType([\n",
    "#                         StructField(\"population_percent\", DoubleType(), True),\n",
    "#                         StructField(\"change_from_prior_day\", IntegerType(), True),\n",
    "#                         StructField(\"seven_day_change_percent\", DoubleType(), True),\n",
    "#                         StructField(\"seven_day_average\", IntegerType(), True)\n",
    "#                     ]))\n",
    "#                 ]))\n",
    "#             ])),\n",
    "#             StructField(\"on_ventilator\", StructType([\n",
    "#                 StructField(\"currently\", StructType([\n",
    "#                     StructField(\"value\", IntegerType(), True),\n",
    "#                     StructField(\"calculated\", StructType([\n",
    "#                         StructField(\"population_percent\", DoubleType(), True),\n",
    "#                         StructField(\"change_from_prior_day\", IntegerType(), True),\n",
    "#                         StructField(\"seven_day_change_percent\", DoubleType(), True),\n",
    "#                         StructField(\"seven_day_average\", IntegerType(), True)\n",
    "#                     ]))\n",
    "#                 ]))\n",
    "#             ]))\n",
    "#         ])),\n",
    "#         StructField(\"death\", StructType([\n",
    "#             StructField(\"total\", StructType([\n",
    "#                 StructField(\"value\", IntegerType(), True),\n",
    "#                 StructField(\"calculated\", StructType([\n",
    "#                     StructField(\"population_percent\", DoubleType(), True),\n",
    "#                     StructField(\"change_from_prior_day\", IntegerType(), True),\n",
    "#                     StructField(\"seven_day_change_percent\", DoubleType(), True),\n",
    "#                     StructField(\"seven_day_average\", IntegerType(), True)\n",
    "#                 ]))\n",
    "#             ]))\n",
    "#         ]))\n",
    "#     ]))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.json(\"/home/infernape/gcp-projects/covid19datapipeline/test/test.json\", schema = schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
